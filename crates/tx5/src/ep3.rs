//! Module containing tx5 endpoint version 3 types.

use crate::deps::lair_keystore_api;
use crate::deps::sodoken;
use crate::BackBuf;
use futures::future::{BoxFuture, Shared};
use lair_keystore_api::prelude::*;
use std::collections::HashMap;
use std::sync::{Arc, Mutex, Weak};
use tx5_core::{Error, EventRecv, EventSend, Id, Result, Tx5Url};

fn next_uniq() -> u64 {
    static UNIQ: std::sync::atomic::AtomicU64 =
        std::sync::atomic::AtomicU64::new(1);
    UNIQ.fetch_add(1, std::sync::atomic::Ordering::Relaxed)
}

type CRes<T> = std::result::Result<T, Error>;

/// Events generated by a tx5 endpoint version 3.
pub enum Ep3Event {
    /// An fatal error indicating the endpoint is no longer viable.
    Error(Error),

    /// Receiving an incoming message from a remote peer.
    Message {
        /// Url of the remote peer.
        peer_url: PeerUrl,

        /// Message sent by the remote peer.
        message: BackBuf,

        /// Permit counting the bytes allowed in memory on the receive side.
        permit: tokio::sync::OwnedSemaphorePermit,
    },
}

impl From<Error> for Ep3Event {
    fn from(err: Error) -> Self {
        Self::Error(err)
    }
}

/// A signal server url.
pub type SigUrl = Tx5Url;

/// A peer connection url.
pub type PeerUrl = Tx5Url;

type SharedSig = Shared<BoxFuture<'static, CRes<Arc<Sig>>>>;
type SigMap = HashMap<SigUrl, (u64, SharedSig)>;

/// Tx5 endpoint version 3 configuration.
pub struct Config3 {
    /// Maximum count of open connections. Default 255.
    pub connection_count_max: u32,

    /// Maximum bytes in memory for any given connection. Default 16 MiB.
    pub connection_bytes_max: u32,

    /// Default timeout for network operations. Default 60 seconds.
    pub timeout: std::time::Duration,
}

impl Default for Config3 {
    fn default() -> Self {
        Self {
            connection_count_max: 255,
            connection_bytes_max: 16 * 1024 * 1024,
            timeout: std::time::Duration::from_secs(60),
        }
    }
}

struct EpShared {
    config: Arc<Config3>,
    this_id: Id,
    ep_uniq: u64,
    lair_tag: Arc<str>,
    lair_client: LairClient,
    sig_limit: Arc<tokio::sync::Semaphore>,
    peer_limit: Arc<tokio::sync::Semaphore>,
    weak_sig_map: Weak<Mutex<SigMap>>,
    evt_send: EventSend<Ep3Event>,
}

/// Tx5 endpoint version 3.
pub struct Ep3 {
    ep: Arc<EpShared>,
    _lair_keystore: lair_keystore_api::in_proc_keystore::InProcKeystore,
    _sig_map: Arc<Mutex<SigMap>>,
    listen_sigs: Arc<Mutex<Vec<tokio::task::JoinHandle<()>>>>,
}

impl Drop for Ep3 {
    fn drop(&mut self) {
        let handles = std::mem::take(&mut *self.listen_sigs.lock().unwrap());
        for handle in handles {
            handle.abort();
        }
    }
}

impl Ep3 {
    /// Construct a new tx5 endpoint version 3.
    pub async fn new(config: Arc<Config3>) -> (Self, EventRecv<Ep3Event>) {
        let sig_limit = Arc::new(tokio::sync::Semaphore::new(
            config.connection_count_max as usize,
        ));

        let peer_limit = Arc::new(tokio::sync::Semaphore::new(
            config.connection_count_max as usize,
        ));

        let lair_tag: Arc<str> =
            rand_utf8::rand_utf8(&mut rand::thread_rng(), 32).into();

        let passphrase = sodoken::BufRead::new_no_lock(
            rand_utf8::rand_utf8(&mut rand::thread_rng(), 32).as_bytes(),
        );

        // this is a memory keystore,
        // so weak persistence security is okay,
        // since it will not be persisted.
        // The private keys will still be mem_locked
        // so they shouldn't be swapped to disk.
        let keystore_config = PwHashLimits::Minimum
            .with_exec(|| LairServerConfigInner::new("/", passphrase.clone()))
            .await
            .unwrap();

        let _lair_keystore = PwHashLimits::Minimum
            .with_exec(|| {
                lair_keystore_api::in_proc_keystore::InProcKeystore::new(
                    Arc::new(keystore_config),
                    lair_keystore_api::mem_store::create_mem_store_factory(),
                    passphrase,
                )
            })
            .await
            .unwrap();

        let lair_client = _lair_keystore.new_client().await.unwrap();

        let seed = lair_client
            .new_seed(lair_tag.clone(), None, false)
            .await
            .unwrap();

        let this_id = Id(*seed.x25519_pub_key.0);

        let (evt_send, evt_recv) = EventSend::new(1024);

        let sig_map = Arc::new(Mutex::new(HashMap::new()));
        let weak_sig_map = Arc::downgrade(&sig_map);

        let this = Self {
            ep: Arc::new(EpShared {
                config,
                this_id,
                ep_uniq: next_uniq(),
                lair_tag,
                lair_client,
                sig_limit,
                peer_limit,
                weak_sig_map,
                evt_send,
            }),
            _lair_keystore,
            _sig_map: sig_map,
            listen_sigs: Arc::new(Mutex::new(Vec::new())),
        };

        (this, evt_recv)
    }

    /// Establish a listening connection to a signal server,
    /// from which we can accept incoming remote connections.
    /// Returns the client url at which this endpoint may now be addressed.
    pub fn listen(&self, sig_url: SigUrl) -> Result<PeerUrl> {
        if !sig_url.is_server() {
            return Err(Error::str("Expected SigUrl, got PeerUrl"));
        }

        let ep = self.ep.clone();
        let peer_url = sig_url.to_client(ep.this_id);

        self.listen_sigs
            .lock()
            .unwrap()
            .push(tokio::task::spawn(async move {
                const B_START: std::time::Duration =
                    std::time::Duration::from_secs(5);
                const B_MAX: std::time::Duration =
                    std::time::Duration::from_secs(60);
                let mut backoff = B_START;
                loop {
                    if assert_sig(&ep, &sig_url).await.is_ok() {
                        // if the conn is still open it's essentially
                        // a no-op to assert it again, so it's
                        // okay to do that quickly.
                        backoff = B_START;
                    } else {
                        backoff *= 2;
                        if backoff > B_MAX {
                            backoff = B_MAX;
                        }
                    }

                    tokio::time::sleep(backoff).await;
                }
            }));

        Ok(peer_url)
    }

    /// Send data to a remote on this tx5 endpoint.
    /// The future returned from this method will resolve when
    /// the data is handed off to our networking backend.
    pub async fn send(
        &self,
        peer_url: PeerUrl,
        data: Vec<BackBuf>,
    ) -> Result<()> {
        if !peer_url.is_client() {
            return Err(Error::str("Expected PeerUrl, got SigUrl"));
        }

        let sig_url = peer_url.to_server();
        let peer_id = peer_url.id().unwrap();

        let sig = assert_sig(&self.ep, &sig_url).await?;

        let peer = sig
            .assert_peer(peer_url, peer_id, PeerDir::ActiveOrOutgoing)
            .await?;

        peer.send(data).await
    }
}

async fn assert_sig(ep: &Arc<EpShared>, sig_url: &SigUrl) -> CRes<Arc<Sig>> {
    let sig_map = match ep.weak_sig_map.upgrade() {
        Some(sig_map) => sig_map,
        None => {
            return Err(Error::str(
                "Signal connection failed due to closed endpoint",
            )
            .into())
        }
    };

    let (_sig_uniq, fut) = sig_map
        .lock()
        .unwrap()
        .entry(sig_url.clone())
        .or_insert_with(|| {
            let sig_uniq = next_uniq();
            let sig_url = sig_url.clone();
            let ep = ep.clone();
            (
                sig_uniq,
                futures::future::FutureExt::shared(
                    futures::future::FutureExt::boxed(async move {
                        tokio::time::timeout(
                            ep.config.timeout,
                            Sig::new(ep, sig_uniq, sig_url),
                        )
                        .await
                        .map_err(|_| {
                            Error::str(
                                "Timeout awaiting signal server connection",
                            )
                        })?
                    }),
                ),
            )
        })
        .clone();

    fut.await
}

fn close_sig(
    weak_sig_map: &Weak<Mutex<SigMap>>,
    sig_url: &SigUrl,
    close_sig_uniq: u64,
) {
    let mut tmp = None;

    if let Some(sig_map) = weak_sig_map.upgrade() {
        let mut lock = sig_map.lock().unwrap();
        if let Some((sig_uniq, sig)) = lock.remove(sig_url) {
            if close_sig_uniq != sig_uniq {
                // most of the time we'll be closing the real one,
                // so optimize for that case, and cause a hash probe
                // in the less likely case some race caused us to
                // try to remove the wrong one.
                tmp = lock.insert(sig_url.clone(), (sig_uniq, sig));
            } else {
                tmp = Some((sig_uniq, sig));
            }
        }
    }

    // make sure nothing is dropped while we're holding the mutex lock
    drop(tmp);
}

enum PeerCmd {
    Error(Error),
    SigRecvIce(serde_json::Value),
}

impl From<Error> for PeerCmd {
    fn from(err: Error) -> Self {
        Self::Error(err)
    }
}

type PeerCmdSend = EventSend<PeerCmd>;
type AnswerSend =
    Arc<Mutex<Option<tokio::sync::oneshot::Sender<serde_json::Value>>>>;
type SharedPeer = Shared<BoxFuture<'static, CRes<Arc<Peer>>>>;
type PeerMap = HashMap<Id, (u64, PeerCmdSend, AnswerSend, SharedPeer)>;

struct SigShared {
    ep: Arc<EpShared>,
    weak_sig: Weak<Sig>,
    sig_uniq: u64,
    sig_url: SigUrl,
    weak_peer_map: Weak<Mutex<PeerMap>>,
}

impl std::ops::Deref for SigShared {
    type Target = Arc<EpShared>;

    fn deref(&self) -> &Self::Target {
        &self.ep
    }
}

struct Sig {
    sig: Arc<SigShared>,
    _permit: tokio::sync::OwnedSemaphorePermit,
    recv_task: tokio::task::JoinHandle<()>,
    ice_servers: Arc<serde_json::Value>,
    peer_map: Arc<Mutex<PeerMap>>,
    sig_cli: tx5_signal::Cli,
}

impl Drop for Sig {
    fn drop(&mut self) {
        tracing::info!(%self.sig.ep_uniq, %self.sig.sig_uniq, %self.sig.sig_url, "Signal Connection Close");

        self.recv_task.abort();

        close_sig(&self.sig.weak_sig_map, &self.sig.sig_url, self.sig.sig_uniq);
    }
}

impl std::ops::Deref for Sig {
    type Target = tx5_signal::Cli;

    fn deref(&self) -> &Self::Target {
        &self.sig_cli
    }
}

impl Sig {
    pub async fn new(
        ep: Arc<EpShared>,
        sig_uniq: u64,
        sig_url: SigUrl,
    ) -> CRes<Arc<Self>> {
        tracing::info!(%ep.ep_uniq, %sig_uniq, %sig_url, "Signal Connection Connecting");

        let _permit =
            ep.sig_limit.clone().acquire_owned().await.map_err(|_| {
                Error::str(
                    "Endpoint closed while acquiring signal connection permit",
                )
            })?;

        let (sig_cli, mut sig_recv) = tx5_signal::Cli::builder()
            .with_lair_tag(ep.lair_tag.clone())
            .with_lair_client(ep.lair_client.clone())
            .with_url(sig_url.to_string().parse().unwrap())
            .build()
            .await?;

        let peer_url = Tx5Url::new(sig_cli.local_addr())?;
        if peer_url.id().unwrap() != ep.this_id {
            return Err(Error::str("Invalid signal server peer Id").into());
        }

        let ice_servers = sig_cli.ice_servers();

        let peer_map: Arc<Mutex<PeerMap>> =
            Arc::new(Mutex::new(HashMap::new()));
        let weak_peer_map = Arc::downgrade(&peer_map);

        Ok(Arc::new_cyclic(move |weak_sig: &Weak<Sig>| {
            let recv_task = {
                let ep = ep.clone();
                let weak_sig = weak_sig.clone();
                let sig_url = sig_url.clone();
                tokio::task::spawn(async move {
                    while let Some(msg) = sig_recv.recv().await {
                        use tx5_signal::SignalMsg::*;
                        match msg {
                            Demo { .. } => (),
                            Offer { rem_pub, offer } => {
                                tracing::trace!(%ep.ep_uniq, %sig_uniq, ?rem_pub, ?offer, "Sig Recv Offer");
                                if let Some(sig) = weak_sig.upgrade() {
                                    let peer_url = sig_url.to_client(rem_pub);
                                    // fire and forget this
                                    tokio::task::spawn(async move {
                                        let _ = sig
                                            .assert_peer(
                                                peer_url,
                                                rem_pub,
                                                PeerDir::Incoming { offer },
                                            )
                                            .await;
                                    });
                                } else {
                                    break;
                                }
                            }
                            Answer { rem_pub, answer } => {
                                tracing::trace!(%ep.ep_uniq, %sig_uniq, ?rem_pub, ?answer, "Sig Recv Answer");
                                if let Some(peer_map) = weak_peer_map.upgrade()
                                {
                                    let r = peer_map
                                        .lock()
                                        .unwrap()
                                        .get(&rem_pub)
                                        .cloned();
                                    if let Some((_, _, answer_send, _)) = r {
                                        let r =
                                            answer_send.lock().unwrap().take();
                                        if let Some(answer_send) = r {
                                            let _ = answer_send.send(answer);
                                        }
                                    }
                                } else {
                                    break;
                                }
                            }
                            Ice { rem_pub, ice } => {
                                tracing::trace!(%ep.ep_uniq, %sig_uniq, ?rem_pub, ?ice, "Sig Recv ICE");
                                if let Some(peer_map) = weak_peer_map.upgrade()
                                {
                                    let r = peer_map
                                        .lock()
                                        .unwrap()
                                        .get(&rem_pub)
                                        .cloned();
                                    if let Some((_, peer_cmd_send, _, _)) = r {
                                        if let Some(permit) =
                                            peer_cmd_send.try_permit()
                                        {
                                            if peer_cmd_send
                                                .send_permit(
                                                    PeerCmd::SigRecvIce(ice),
                                                    permit,
                                                )
                                                .is_err()
                                            {
                                                break;
                                            }
                                        } else {
                                            break;
                                        }
                                    }
                                } else {
                                    break;
                                }
                            }
                        }
                    }

                    close_sig(&ep.weak_sig_map, &sig_url, sig_uniq);
                })
            };

            let weak_peer_map = Arc::downgrade(&peer_map);

            tracing::info!(%ep.ep_uniq, %sig_uniq, %sig_url, "Signal Connection Open");

            Self {
                sig: Arc::new(SigShared {
                    ep,
                    weak_sig: weak_sig.clone(),
                    sig_uniq,
                    sig_url,
                    weak_peer_map,
                }),
                _permit,
                recv_task,
                ice_servers,
                peer_map,
                sig_cli,
            }
        }))
    }

    async fn assert_peer(
        &self,
        peer_url: PeerUrl,
        peer_id: Id,
        peer_dir: PeerDir,
    ) -> CRes<Arc<Peer>> {
        if peer_id == self.sig.this_id {
            return Err(Error::str("Cannot establish connection with remote peer id matching this id").into());
        }

        let mut tmp = None;

        let (_peer_uniq, _peer_cmd_send, _answer_send, fut) = {
            let mut lock = self.peer_map.lock().unwrap();

            if peer_dir.is_incoming() && lock.contains_key(&peer_id) {
                // we need to check negotiation
                if peer_id > self.sig.this_id {
                    // we are the polite node, drop our existing connection
                    tmp = lock.remove(&peer_id);
                }
                // otherwise continue on to return the currently
                // registered connection because we're the impolite node.
            }

            lock.entry(peer_id)
                .or_insert_with(|| {
                    let mut answer_send = None;
                    let new_peer_dir = match peer_dir {
                        PeerDir::ActiveOrOutgoing => {
                            let (s, r) = tokio::sync::oneshot::channel();
                            answer_send = Some(s);
                            NewPeerDir::Outgoing { answer_recv: r }
                        }
                        PeerDir::Incoming { offer } => {
                            NewPeerDir::Incoming { offer }
                        }
                    };
                    let sig = self.sig.clone();
                    let peer_uniq = next_uniq();
                    let ice_servers = self.ice_servers.clone();
                    let (peer_cmd_send, peer_cmd_recv) = EventSend::new(1024);
                    (
                        peer_uniq,
                        peer_cmd_send,
                        Arc::new(Mutex::new(answer_send)),
                        futures::future::FutureExt::shared(
                            futures::future::FutureExt::boxed(async move {
                                tokio::time::timeout(
                                    sig.config.timeout,
                                    Peer::new(
                                        sig,
                                        peer_url,
                                        peer_id,
                                        peer_uniq,
                                        ice_servers,
                                        new_peer_dir,
                                        peer_cmd_recv,
                                    ),
                                )
                                .await
                                .map_err(
                                    |_| {
                                        Error::str(
                                            "Timeout awaiting peer connection",
                                        )
                                    },
                                )?
                            }),
                        ),
                    )
                })
                .clone()
        };

        // make sure to drop this after releasing our mutex lock
        drop(tmp);

        fut.await
    }
}

fn close_peer(
    weak_peer_map: &Weak<Mutex<PeerMap>>,
    peer_id: Id,
    close_peer_uniq: u64,
) {
    let mut tmp = None;

    if let Some(peer_map) = weak_peer_map.upgrade() {
        let mut lock = peer_map.lock().unwrap();
        if let Some((peer_uniq, cmd, ans, peer)) = lock.remove(&peer_id) {
            if close_peer_uniq != peer_uniq {
                // most of the time we'll be closing the real one,
                // so optimize for that case, and cause a hash probe
                // in the less likely case some race caused us to
                // try to remove the wrong one.
                tmp = lock.insert(peer_id, (peer_uniq, cmd, ans, peer));
            } else {
                tmp = Some((peer_uniq, cmd, ans, peer));
            }
        }
    }

    // make sure nothing is dropped while we're holding the mutex lock
    drop(tmp);
}

enum PeerDir {
    ActiveOrOutgoing,
    Incoming { offer: serde_json::Value },
}

impl PeerDir {
    fn is_incoming(&self) -> bool {
        matches!(self, PeerDir::Incoming { .. })
    }
}

enum NewPeerDir {
    Outgoing {
        answer_recv: tokio::sync::oneshot::Receiver<serde_json::Value>,
    },
    Incoming {
        offer: serde_json::Value,
    },
}

struct Peer {
    sig: Arc<SigShared>,
    peer_id: Id,
    peer_uniq: u64,
    _permit: tokio::sync::OwnedSemaphorePermit,
    cmd_task: tokio::task::JoinHandle<()>,
    recv_task: tokio::task::JoinHandle<()>,
    data_task: tokio::task::JoinHandle<()>,
    #[allow(dead_code)]
    peer: Arc<tx5_go_pion::PeerConnection>,
    data_chan: Arc<tx5_go_pion::DataChannel>,
    send_limit: Arc<tokio::sync::Semaphore>,
}

impl Drop for Peer {
    fn drop(&mut self) {
        tracing::info!(%self.sig.ep_uniq, %self.sig.sig_uniq, %self.peer_uniq, ?self.peer_id, "Peer Connection Close");

        self.cmd_task.abort();
        self.recv_task.abort();
        self.data_task.abort();

        close_peer(&self.sig.weak_peer_map, self.peer_id, self.peer_uniq);
    }
}

impl Peer {
    pub async fn new(
        sig: Arc<SigShared>,
        peer_url: PeerUrl,
        peer_id: Id,
        peer_uniq: u64,
        ice_servers: Arc<serde_json::Value>,
        new_peer_dir: NewPeerDir,
        mut peer_cmd_recv: EventRecv<PeerCmd>,
    ) -> CRes<Arc<Self>> {
        tracing::info!(%sig.ep_uniq, %sig.sig_uniq, %peer_uniq, ?peer_id, "Peer Connection Connecting");

        let _permit =
            sig.peer_limit.clone().acquire_owned().await.map_err(|_| {
                Error::str(
                    "Endpoint closed while acquiring peer connection permit",
                )
            })?;

        let sig_hnd = match sig.weak_sig.upgrade() {
            None => {
                return Err(Error::str(
                    "Sig shutdown while opening peer connection",
                )
                .into())
            }
            Some(sig_hnd) => sig_hnd,
        };

        let peer_config = BackBuf::from_json(ice_servers)?;

        let (peer, mut peer_recv) = tx5_go_pion::PeerConnection::new(
            peer_config.imp.buf,
            Arc::new(tokio::sync::Semaphore::new(
                sig.config.connection_bytes_max as usize,
            )),
        )
        .await?;

        let peer = Arc::new(peer);

        let (chan_send, chan_recv) = tokio::sync::oneshot::channel();
        let mut chan_send = Some(chan_send);

        match new_peer_dir {
            NewPeerDir::Outgoing { answer_recv } => {
                let chan = peer
                    .create_data_channel(tx5_go_pion::DataChannelConfig {
                        label: Some("data".into()),
                    })
                    .await?;

                if let Some(chan_send) = chan_send.take() {
                    let _ = chan_send.send(chan);
                }

                let mut offer = peer
                    .create_offer(tx5_go_pion::OfferConfig::default())
                    .await?;

                let offer_json = offer.as_json()?;

                tracing::debug!(?offer_json, "create_offer");

                sig_hnd.offer(peer_id, offer_json).await?;

                peer.set_local_description(offer).await?;

                let answer = answer_recv.await.map_err(|_| {
                    Error::str("Failed to receive answer on peer connect")
                })?;
                let answer = BackBuf::from_json(answer)?;

                peer.set_remote_description(answer.imp.buf).await?;
            }
            NewPeerDir::Incoming { offer } => {
                let offer = BackBuf::from_json(offer)?;

                peer.set_remote_description(offer.imp.buf).await?;

                let mut answer = peer
                    .create_answer(tx5_go_pion::AnswerConfig::default())
                    .await?;

                let answer_json = answer.as_json()?;

                tracing::debug!(?answer_json, "create_answer");

                sig_hnd.answer(peer_id, answer_json).await?;

                peer.set_local_description(answer).await?;
            }
        }

        let cmd_task = {
            let weak_peer = Arc::downgrade(&peer);
            let sig = sig.clone();
            tokio::task::spawn(async move {
                while let Some(cmd) = peer_cmd_recv.recv().await {
                    match cmd {
                        PeerCmd::Error(err) => {
                            tracing::warn!(?err);
                            break;
                        }
                        PeerCmd::SigRecvIce(ice) => {
                            if let Some(peer) = weak_peer.upgrade() {
                                if let Ok(ice) = BackBuf::from_json(ice) {
                                    if let Err(err) = peer
                                        .add_ice_candidate(ice.imp.buf)
                                        .await
                                    {
                                        tracing::trace!(?err);
                                    }
                                }
                            } else {
                                break;
                            }
                        }
                    }
                }

                close_peer(&sig.weak_peer_map, peer_id, peer_uniq);
            })
        };

        let recv_task = {
            let sig = sig.clone();
            tokio::task::spawn(async move {
                while let Some(evt) = peer_recv.recv().await {
                    use tx5_go_pion::PeerConnectionEvent as Evt;
                    match evt {
                        Evt::Error(err) => {
                            tracing::warn!(?err);
                            break;
                        }
                        Evt::State(_state) => (),
                        Evt::ICECandidate(mut ice) => {
                            let ice = match ice.as_json() {
                                Err(err) => {
                                    tracing::warn!(?err, "invalid ice");
                                    break;
                                }
                                Ok(ice) => ice,
                            };
                            if let Some(sig_hnd) = sig.weak_sig.upgrade() {
                                if sig_hnd.ice(peer_id, ice).await.is_err() {
                                    break;
                                }
                            } else {
                                break;
                            }
                        }
                        Evt::DataChannel(c, r) => {
                            if let Some(chan_send) = chan_send.take() {
                                let _ = chan_send.send((c, r));
                            } else {
                                tracing::warn!("Invalid incoming data channel");
                                break;
                            }
                        }
                    }
                }

                close_peer(&sig.weak_peer_map, peer_id, peer_uniq);
            })
        };

        let (data_chan, mut data_recv) = chan_recv.await.map_err(|_| {
            Error::str("Failed to establish peer connection data channel")
        })?;

        let data_chan = Arc::new(data_chan);

        let data_task = {
            let sig = sig.clone();
            tokio::task::spawn(async move {
                while let Some(evt) = data_recv.recv().await {
                    use tx5_go_pion::DataChannelEvent::*;
                    match evt {
                        Error(err) => {
                            tracing::warn!(?err);
                            break;
                        }
                        Open => (),
                        Close => break,
                        Message(message, permit) => {
                            let message = BackBuf::from_raw(message);
                            if sig
                                .evt_send
                                .send(Ep3Event::Message {
                                    peer_url: peer_url.clone(),
                                    message,
                                    permit,
                                })
                                .await
                                .is_err()
                            {
                                break;
                            }
                        }
                        BufferedAmountLow => (),
                    }
                }

                close_peer(&sig.weak_peer_map, peer_id, peer_uniq);
            })
        };

        let mut ready_state = data_chan.ready_state()?;
        let mut backoff = std::time::Duration::from_millis(10);

        loop {
            if ready_state >= 2 {
                break;
            }

            tokio::time::sleep(backoff).await;
            backoff *= 2;
            ready_state = data_chan.ready_state()?;
        }

        if ready_state > 2 {
            return Err(Error::str(
                "Data channel closed while connecting peer",
            )
            .into());
        }

        tracing::info!(%sig.ep_uniq, %sig.sig_uniq, %peer_uniq, ?peer_id, "Peer Connection Open");

        Ok(Arc::new(Self {
            sig,
            peer_id,
            peer_uniq,
            _permit,
            cmd_task,
            recv_task,
            data_task,
            peer,
            data_chan,
            send_limit: Arc::new(tokio::sync::Semaphore::new(1)),
        }))
    }

    pub async fn send(&self, data: Vec<BackBuf>) -> Result<()> {
        // size 1 semaphore makes sure blocks of messages are contiguous
        let _permit = self
            .send_limit
            .acquire()
            .await
            .map_err(|_| Error::str("Failed to acquire send permit"))?;

        for mut buf in data {
            if buf.len()? > 16 * 1024 {
                return Err(Error::str("Buffer cannot be larger than 16 KiB"));
            }

            let mut backoff = std::time::Duration::from_millis(1);

            loop {
                if self.data_chan.buffered_amount()?
                    <= self.sig.config.connection_bytes_max as usize
                {
                    break;
                }

                tokio::time::sleep(backoff).await;
                backoff *= 2;
            }

            self.data_chan.send(buf.imp.buf).await?;
        }

        Ok(())
    }
}

#[cfg(test)]
mod ep3_tests {
    use super::*;

    fn init_tracing() {
        let subscriber = tracing_subscriber::FmtSubscriber::builder()
            .with_env_filter(
                tracing_subscriber::filter::EnvFilter::from_default_env(),
            )
            .with_file(true)
            .with_line_number(true)
            .finish();
        let _ = tracing::subscriber::set_global_default(subscriber);
    }

    #[tokio::test(flavor = "multi_thread")]
    async fn ep3_sanity() {
        init_tracing();

        let mut srv_config = tx5_signal_srv::Config::default();
        srv_config.port = 0;

        let (srv_driver, addr_list, _) =
            tx5_signal_srv::exec_tx5_signal_srv(srv_config).unwrap();
        let sig_task = tokio::task::spawn(srv_driver);

        let sig_port = addr_list.get(0).unwrap().port();

        let sig_url =
            Tx5Url::new(format!("ws://localhost:{}", sig_port)).unwrap();
        println!("sig_url: {sig_url}");

        let (ep1, _) = Ep3::new(Arc::new(Config3::default())).await;

        let cli_url1 = ep1.listen(sig_url.clone()).unwrap();
        println!("cli_url1: {cli_url1}");

        let (ep2, mut ep2_recv) = Ep3::new(Arc::new(Config3::default())).await;

        let cli_url2 = ep2.listen(sig_url).unwrap();
        println!("cli_url2: {cli_url2}");

        ep1.send(cli_url2, vec![BackBuf::from_slice(b"hello").unwrap()])
            .await
            .unwrap();

        let res = ep2_recv.recv().await.unwrap();
        match res {
            Ep3Event::Message { mut message, .. } => {
                assert_eq!(&b"hello"[..], &message.to_vec().unwrap());
            }
            _ => panic!(),
        }

        println!("drop1");
        drop(ep1);
        println!("drop2");
        drop(ep2);

        println!("abort sig");
        sig_task.abort();

        println!("all done.");
    }
}
